{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 1\n",
    "* CSCI-5931 : Deep Learning\n",
    "* Spring 2024\n",
    "* Instructor: Ashis Kumer Biswas\n",
    "* Student name: Bhavishya Vudatha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little Background about the problem\n",
    "\n",
    "`Customer churn`` occurs when customers stop doing business with a company, also known as customer attrition. It is also referred to as loss of clients or customers.\n",
    "\n",
    "You are given sensitive information of 9,000 of an European Bank, EBQ. Your task is to build an Artificial Neural Network (ANN) based on the dataset such that later the ANN model can predict correctly who is going to leave next. This predictive analysis is vital for the EBQ bank to revise their business strategy towards customer retention. What do you think?\n",
    "\n",
    "Anyway, you are recruited by the bank to do the data science. And, the head of the bank only trusts heads, i.e., brains…. I mean neural networks for making any decisions. And luckily you were in Dr. B’s class and you know something(?) about the ANN that you could successfully convince the head of the bank during the interview. He has put a lot of faith in you. Now, can you solve his problem?\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Please download the zip file, `PA1-deliverables.zip``. Unzip it in your workspace. Here below is the file hierarchy of \"PA1-deliverables/\" folder:\n",
    "\n",
    "```\n",
    "PA1-deliverables\n",
    "├── 2024-Spring-DL-PA1-assignment.ipynb\n",
    "├── dataset\n",
    "│   └── datasetX.csv\n",
    "├── figures\n",
    "│   ├── le.png\n",
    "│   ├── nn-1.png\n",
    "│   ├── nn-1.svg\n",
    "│   ├── nn-2.png\n",
    "│   ├── nn-2.svg\n",
    "│   ├── nn-3.png\n",
    "│   ├── nn-3.svg\n",
    "│   └── ohe.png\n",
    "└── saved_models\n",
    "```\n",
    "As you can see you will mostly be working with the 2023-Fall-DL-PA1-assignment.ipynb, i.e., the jupyter notebook. The notebook accesses the dataset files: `dataset/datasetX.csv` containing few customer information and is labeled (i.e., the target column, `Exited` is present). Here below is a brief summary of the features you will find in the datasets:\n",
    "\n",
    "* `CustomerId`: a unique identifier for each customer within the dataset. These values are not ordered sequentially within the dataset, and are only used to identify a specific customer. It typically does not have any influence to whether a customer leaves the business.\n",
    "* `Surname`: A string used to identify the customer in the dataset. Surname may be distinct amidst all or most customers. Because of this, it most likely won't affect the target variable. \n",
    "* `CreditScore`: a numeric representation of the customer's individual fiscal credit score. Typically used to indicate eligibility for loans. Current credit scores use a range from 300 to 850, but the FICO auto score range uses 250-900. This feature likely determines retention rate of customers. \n",
    "* `Geography`: this feature contains a categorical string representing the name of a country the customer is from originally. \n",
    "* `Gender`: this feature contains a categorical string representing the gender of the customer (\"Male\"/\"Female\"). \n",
    "* `Age`: a numerical integer representation of a customer's age. Intuition suggests that older customers are likely to have higher retention than younger customers.\n",
    "* `Tenure`: a numerical integer representation. It is assumed that this feature represents the number of total years the customer has been retained. It is likely that customers which have been retained longer will continue to be retained.\n",
    "* `Balance`: a numerical floating point number (to two decimal places of precision) indicating the customer's current bank balance (assumed total across all accounts). Customers with a greater balance may be less likely to exit the account due to difficulty of transfer. \n",
    "* `NumOfProducts`: numeric integer value. It is assumed that this value represents the number of accounts (products) that this customer has open. Further evaluation of this feature would be needed to determine the usefulness of this feature, but at face-value, intuition dictates that a customer with more products is less likely to exit. \n",
    "* `HasCrCard`: boolean flag (0 or 1) representing whether the customer has a credit card or not. \n",
    "* `IsActiveMember`: boolean flag (0 or 1) representing whether the customer is an active member of the bank. It is assumed this indicates whether the customer has transactions on the regular banking statement. Intuition dictates that inactive members are more likely to exit. \n",
    "* `EstimatedSalary`: numerical floating point representation of the customer's predicted salary (to two decomal places) intuition dictates that customers with different incomes may behave differently with respect to retention rate. \n",
    "* `Exited`: boolean flag (0 or 1) representing whether the customer has exited their account. This is the target variable for the dataset. It should not be dropped, but should not be included as the training input (X), and should instead be separated as the target label (y). \n",
    "\n",
    "You will also see an empty directory `saved_models/`, that is for you to save all the models you'd train in this assignment.\n",
    "\n",
    "`figures/` directory contains few image files used to properly document this assignment. Please do not delete and when possible please move them with this jupyter notebook for proper display of its contents.\n",
    "\n",
    "> In this Jupyter notebook please write your solutions / codes in the cells marked with `#Your solution goes here...`. You may add additional code cells after that cell if you desire. But, please do not remove any cell originally given in the notebook.\n",
    "\n",
    "> After you solve the assignment in the jupyter notebook, be sure to execute and save it so that execution/results/printouts are also saved with it.\n",
    "> Finally, submit only the saved jupyter notebook (`2024-Spring-DL-PA1-assignment.ipynb`) in Canvas to receive grade. For this assignment, Canvas only will accept the jupyter notebook in \"*.ipynb\" extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : (10 points)\n",
    "* Define a function named `summarize_dataset` that takes only one argument: `csv_file`, where `csv_file` is the name of the given `csv` file with this assignment, i.e., `datasetX.csv`. \n",
    "  * The function is expected to summarize the given dataset in the following way:\n",
    "```\n",
    "total number of rows = a\n",
    "total number of columns = b\n",
    "number of columns having non-numeric values = c\n",
    "columns with missing values = [ (d1, e1)  (d2, e2), ... ]\n",
    "gender based summary of exited column = [ (f1, g1)  (f2, g2), ... ]\n",
    "age based summary of exited column = [ ('below or equal to 40', h1)  ('above 40', h2) ]\n",
    "credit score summary =  i +/- j \n",
    "```\n",
    "  \n",
    "where,\n",
    "\n",
    "* `a` is total number of rows in the dataset.\n",
    "* `b` is total number of columns in the dataset.\n",
    "* `c` is number of columns having non-numeric values.\n",
    "* $(d_i, e_i)$ (i.e., a pair/tuple entry) represents column name ($d_i$) and number of missing values present in that column ($e_i$). If number of missing values in a column is zero (0), you do not need to list it. Please sort the tuple entries in descending order of $e_i$ values.\n",
    "* $g_i$ represents the percentage of gender $f_i$ who exited. Please sort the tuple entries entries in descending order of $g_i$ values. Also, print the percentages in 2 decimal places after the decimal point, and print use `%` symbol after the percentage value.\n",
    "* $h_1$ and $h_2$ represents the percentage of $\\leq 40$ year olds who exited, and the percentage $>40$ year olds who exited.  Also, print the percentages in 2 decimal places after the decimal point, and print use `%` symbol after the percentage value.\n",
    "* `j` and `k` are average and standard deviation of credit scores among the data samples respectively. Please print the way it is shown above. Also, print the both values in 2 decimal places after the decimal point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows = 9000\n",
      "Total number of columns = 13\n",
      "Number of columns having non-numeric values = 3\n",
      "Columns with missing values = [('Age', 397), ('CreditScore', 26)]\n",
      "Gender based summary of exited column = [('Female', '24.77%'), ('Male', '16.63%')]\n",
      "Age based summary of exited column = [('below or equal to 40', '10.94%'), ('above 40', '37.63%')]\n",
      "Credit score summary = 650.25 +/- 96.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def summarize_dataset(file):\n",
    "    df=pd.read_csv(file)\n",
    "    \n",
    "    #total number of rows\n",
    "    a=len(df)\n",
    "    \n",
    "    #total number of columns\n",
    "    b=len(df.columns)\n",
    "    \n",
    "    # number of columns having non-numeric values\n",
    "    c=df.select_dtypes(exclude=['number']).shape[1]\n",
    "    \n",
    "    \n",
    "     # Columns with missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing = missing.loc[missing > 0].sort_values(ascending=False)\n",
    "    d = list(zip(missing.index, missing.values))\n",
    "    \n",
    "    \n",
    "    # Gender based summary of exited column\n",
    "    gender = df.groupby('Gender')['Exited'].mean() * 100\n",
    "    gender = gender.round(2).reset_index()\n",
    "    f = list(zip(gender['Gender'], gender['Exited'].astype(str) + '%'))\n",
    "    \n",
    "    # Age based summary of exited column\n",
    "    age = df.groupby(pd.cut(df['Age'], bins=[0, 40, float('inf')]))['Exited'].mean() * 100\n",
    "    age = age.round(2).reset_index()\n",
    "    h1 = age.loc[0, 'Exited']\n",
    "    h2 = age.loc[1, 'Exited']\n",
    "    age_summary = [('below or equal to 40', str(h1) + '%'), ('above 40', str(h2) + '%')]\n",
    "\n",
    "    \n",
    "    # Credit score summary\n",
    "    i = df['CreditScore'].mean()\n",
    "    j = df['CreditScore'].std()\n",
    "    creditscore_summary = f\"{i:.2f} +/- {j:.2f}\"\n",
    "    \n",
    "    # Print the summaries\n",
    "    print(\"Total number of rows =\", a)\n",
    "    print(\"Total number of columns =\", b)\n",
    "    print(\"Number of columns having non-numeric values =\", c)\n",
    "    print(\"Columns with missing values =\", d)\n",
    "    print(\"Gender based summary of exited column =\", f)\n",
    "    print(\"Age based summary of exited column =\", age_summary)\n",
    "    print(\"Credit score summary =\", creditscore_summary)\n",
    "    \n",
    "summarize_dataset(\"datasetX.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "* Preprocessing the given dataset for the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 (10 points)\n",
    "\n",
    "* First preprocessing that we are going to do on the dataset is dropping two features (i.e., columns) that, I think, are irrelevant and would not make any meaningful relationship with the `Exited` feature. The features are: `CustomerId` and `Surname`.\n",
    "* Make sure to create a variable called `dataset_dropped` that will store the revised dataset.\n",
    "* Please print the name of the columns of the revised dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-247a930d79bde8e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance',\n",
       "       'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary',\n",
       "       'Exited'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('datasetX.csv')\n",
    "dataset_dropped = dataset.drop(columns=['CustomerId', 'Surname']).dropna()\n",
    "dataset_dropped.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 (10 points)\n",
    "* Second Preprocessing that we are going to do is *Shuffle Rows* of the dataset obtained from `Task 2.1`.\n",
    "* \"It is extremely important to shuffle the training data, so that you do not obtain entire minibatches of highly correlated examples. As long as the data has been shuffled, everything should work OK. Different random orderings will perform slightly differently from each other but this will be a small factor that does not matter much.\" -- [Ian Goodfellow](https://qr.ae/pGBgw8)\n",
    "* Use a random seed value `4321` in case you will call any stochastic method.\n",
    "* Make sure to create a variable called `dataset_shuffled` that will store the revised dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "shuff_dataset_final",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "random_seed = 4321\n",
    "dataset_shuffled = dataset_dropped.sample(frac=1, random_state=random_seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: (10 points)\n",
    "\n",
    "* Third Preprocessing that we will do is X-y Partitioning of the dataset obtained from `Task 2.2`.\n",
    "* In its current state, the dataset contains both independent (input, `X`) and the target (output, `y`) features within the same dataframe. For ease of of the training process, we need to partition the training features from the target feature into two separate dataframes. \n",
    "* Make sure, the following cell contains at least two variables: `X` and `y`:\n",
    "  * `X` contains part of the dataset with only independent features, and \n",
    "  * `y` having only the dependent/target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset_shuffled.drop(columns=['Exited'])\n",
    "y = dataset_shuffled['Exited']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4 (10 points)\n",
    "* Fourth Preprocessing that we will do is Train-Test Split of X, y obtained from `Task 2.3`.\n",
    "* Now that we have X and y tables with appropriate feature pruning performed, we must split the data into a training partition (`X_train, y_train`) and a testing partition (`X_test, y_test`). \n",
    "* The training partitions (`X_train, y_train`) will be used to train your model, while the test partition (`X_test, y_test`) will be set aside during the training steps, and will only be used to evaluate the trained model. \n",
    "* Training and test splits should be mutually exclusive to the datasets... i.e., a sample can not be both in training and test sets.\n",
    "* Please perform a 80-20 split, meaning 80% of the (X,y) dataset will be in (X_train, y_train) split, while, remaining 20% will be in (X_test,y_test) split. \n",
    "* Please use random seed `4321` prior to calling any stochastic methods.\n",
    "* Make sure the following cell contains at least 4 variables: `X_train`, `y_train`, `X_test`, `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "train_test_split",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5 (10 points)\n",
    "\n",
    "* Fifth preprocessing that we will do is the *Conversion of Categorical features to Numerical*\n",
    "* Please adopt the `One Hot Encoding` method instead of `Label Encoding` while converting the categorical features. \n",
    "* Make sure the following cell contains a variable named `X_train_ohe` that would contain one hot encoded `X_train` data; on the two categorical columns: 'Geography','Gender'. Please save the encoder for later use; e.g., encode `X_test` dataset, or any future test sample given to you. Under any circumstance, you must not encode `X_test` independently like you would do for `X_train`.\n",
    "* Now, encode the `X_test` data using the one hot encoder you saved while you encoded the `X_train`, and name the variable `X_test_ohe`.\n",
    "\n",
    "\n",
    "* **Both encoding techniques are outlined below**:\n",
    "> A little background first: Categorical features are features that contain values that are not numeric. It would be absurd to work with non-numeric features if you ask neurons in your ANN to compute the weighted sum of inputs, and then pass through activation function, right? These maths are undefined. An obvious solution you may be intrigued to do is dropping the features! Aha! Wrong!! Every piece of data is precious... may present with valuable insights of the data samples to find the patterns to map inputs with output/targets. So, we should include them. But, how?\n",
    "\n",
    "The answer is via \"Encoding\". \n",
    "\n",
    "Several types of encodings are used in practice. Here below are just 2 popular ones:\n",
    "1. **Label Encoding**, where labels are encoded as subsequent numbers. Say, for a categorical feature named \"Category\" with three categorical values: {“Cat”, “Dog” or “Zebra”} can be encoded to \"0\", \"1\", \"2\" respectively as in figure below. The issue with this type of encoding may unintentionally impose a type of ordering of the categories, that may add bias to the training.\n",
    "\n",
    "\n",
    "![label-encoding](figures/le.png)\n",
    "\n",
    "2. **One Hot Encoding**, ignores the ordering of the categories all together. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1. Also, don't forget to remove the original categorical features. Here below just an example, how to convert the categorical feature called \"Category\" having the {“Cat”, “Dog” or “Zebra”} values into three new binary features: \"Cat\", \"Dog\", \"Zebra\".\n",
    "\n",
    "![label-encoding](figures/ohe.png)\n",
    "\n",
    "**A note on the Dummy Variable Trap**\n",
    "The Dummy Variable Trap occurs when two or more dummy variables created by one-hot encoding are highly correlated (i.e., becomes multi-collinear). This means that one variable can be predicted from the others, making it difficult to interpret predicted coefficient variables in regression models. In other words, the individual effect of the dummy variables on the prediction model can not be interpreted well because of multicollinearity.\n",
    "\n",
    "Using the one-hot encoding method, a new dummy variable is created for each categorical variable to represent the presence (1) or absence (0) of the categorical variable. For example, if tree species is a categorical variable made up of the values pine, or oak, then tree species can be represented as a dummy variable by converting each variable to a one-hot vector. This means that a separate column is obtained for each category, where the first column represents if the tree is pine and the second column represents if the tree is oak. Each column will contain a 0 or 1 if the tree in question is of the column's species. These two columns are multi-collinear since if a tree is pine, then we know it's not oak and vice versa. The machine learning models trained on dataset having this multi-collinearity suffers. A remedy is to drop first (or any one) of the dummy (i.e., one-hot) features created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "one-hot-encoding-function",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/personal/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "X_train_encoded = encoder.fit_transform(X_train[['Geography', 'Gender']])\n",
    "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(['Geography', 'Gender']))\n",
    "X_train_ohe = pd.concat([X_train.drop(columns=['Geography', 'Gender']).reset_index(drop=True), X_train_encoded_df], axis=1)\n",
    "X_test_encoded = encoder.transform(X_test[['Geography', 'Gender']])\n",
    "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(['Geography', 'Gender']))\n",
    "X_test_ohe = pd.concat([X_test.drop(columns=['Geography', 'Gender']).reset_index(drop=True), X_test_encoded_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6: (10 points)\n",
    "\n",
    "* Sixth Preprocessing that we are going to do is *Normalization of X_train_ohe, and X_test_ohe*\n",
    "\n",
    "* Now that we have all numerical training and test datasets: `X_train_ohe` and `X_test_ohe` respectively, we can normalize each features in both of the datasets. **Normalization** is just one of the way to scale each feature. In class you'll learn a ton of other ways to scale. For this task, let's resort to **Normalization**.\n",
    "\n",
    "> \"The rule of thumb for scaling datasets, is we scale training dataset first, then using the statistics that we learn during the scaling process, we scale the test dataset. We do not learn any new statistics while we scale the test dataset.\"\n",
    "\n",
    "* Also, scaling is commonly performed column-wise, and never sample/row wise.\n",
    "\n",
    "* Make sure the following cell contains the two scaled variables: `X_train_scaled` and `X_test_scaled` based on the requirements mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "normalizer-learning-training",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_ohe)\n",
    "X_test_scaled = scaler.transform(X_test_ohe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: (10 points)\n",
    "* *Designing your first Artificial Neural Network (ANN) based classifier* using i) **Micrograd**, and ii) **Tensorflow** or PyTorch**.:\n",
    "\n",
    "  > **Micrograd** by Andrej Karpathy [[video](https://youtu.be/VMj-3S1tku0?si=D5m1IJW5AkJzhvLE)][[git-prepo](https://github.com/karpathy/micrograd.git)]\n",
    "\n",
    "  > **Keras/Tensorflow** @ Python reference:  please take a look here [https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/). \n",
    "\n",
    "  > **PyTorch** @Python reference: Please take a look at [Deep Learning with PyTorch Guide](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "  \n",
    "  \n",
    "### Step 1: The ANN architecture\n",
    "* Let's design the first artificial network architecture for the classifier we would like to build. Here below is one. How did I get this architecture? Maybe in my dream! Haha. Someday you will get one too. Until that, let's follow the architecture below:\n",
    "  ![Task 3 ANN architecture](figures/nn-1.png)\n",
    "  * **Input layer** will have 11 units as the dimension of training set: `X_train_scaled` (i.e, number of columns = 11).\n",
    "  * **First hidden layer** will have 5 neurons, each with \"Rectified Linear Unit (`ReLU``)\" as activation function.\n",
    "  * **Second hidden layer** will have 4 neurons, each with \"`ReLU`\" as activation function.\n",
    "  * **Output layer** will have just 1 neuron, with `sigmoid`` activation function. \n",
    "    * The reason behind a single neuron with `sigmoid` activation at the output layer is that, output of this neuron will tell the probability score of the target outcome: \"Exited\" True or False. If the output neuron produces value above 0.5, we will say the neural network predicted \"True\", otherwise, False. This is the beauty of using sigmoid function at the output layer as we can interpret the output value of the neuron as probability score.\n",
    "* The architecture will come to life when you initiate the training process with training data.\n",
    "  * The training process needs a g**radient descend based optimizer**, and a convex looking **loss function**.  \n",
    "  * For this task, let's choose the `adam` optimizer, and the `binary_crossentropy` as the loss function.\n",
    "### Step 2: The Training process\n",
    "* Let's start the training process with the training dataset, `X_train_scaled`.\n",
    "  * Gradient descend based optimization updates run in iterations. When number of iterations equal the total number of training samples, we call that `1 epoch` has passed. Let's continue the training for `25 epochs`. But, you are welcome to run longer than this. There are, however, simpler way to determine if you should early stop your training. \n",
    "    * (Optional) Can you extract information about optimization in each epoch? If so, draw a epoch-loss plot, where X-axis needs to show epoch numbers, and Y-axis will show the `binary_crossentropy` loss value in that particular epoch iteration.\n",
    "* Don't forget to save the model into a file in the `saved_models/` directory so that you can re-use it later for further prediction. Let's give it a name: `model-ann-11-5-4-1-xx` with an extension of your choosing, with `xx` must be replaced by any of `{mc,pt,tf}`, where `mc` to denote if that's a micrograd based model that you are saving, or `pt` for a pytorch model, or `tf` for a tensorflow model.\n",
    "\n",
    "### Step 3: The Evaluation\n",
    "\n",
    "#### (part 3.1) Evaluating your model with the entire test dataset:\n",
    "\n",
    "* Load your trained model `model-ann-11-5-4-1-xx` from the file, and have it predict the entire test set you have at head: (`X_test_scaled`). Luckily, for each of the test sample in the set, you also have ground true `Exited` value in the `y_test`. \n",
    "* Please report/print your model's predictive performance on the test set in terms of `accuracy`, `precision`, `recall`, and `F1 scores`.\n",
    "\n",
    "#### (part 3.2) Evaluating your model with 1 test sample with known Exited value\n",
    "\n",
    "* Here is a single test sample for which we know the ground true `Exited` value:\n",
    "\n",
    "| CustomerId | Surname | CreditScore | Geography | Gender | Age | Tenure | Balance | NumOfProducts | HasCrCard | IsActiveMember |EstimatedSalary | Exited |\n",
    "| :---        |    :----:   |          ---: | :---        |    :----:   |          ---: | :---        |    :----:   |          ---: | :---        |    :----:   |          ---: |          ---: |\n",
    "| 55443322 | Reynolds |709|Germany|Male|30|9|115479.48|2|1|1|134732.99|0|\n",
    "\n",
    "* Load your trained model `model-ann-11-5-4-1-xx` from the file, and have it predict the test sample above. Please don't forget to preprocess this test samples so that it is compliant with the input and model requirements.\n",
    "* Please report whether it predicts a 0 or 1 for the `Exited` target, and also comment whether your model makes a mistake or predicts correctly.\n",
    "\n",
    "#### (part 3.3) Evaluating your model with 1 test sample without known Exited value\n",
    "\n",
    "* Here is a single test sample for which we **do not know** the ground true `Exited` value:\n",
    "\n",
    "| CustomerId | Surname | CreditScore | Geography | Gender | Age | Tenure | Balance | NumOfProducts | HasCrCard | IsActiveMember |EstimatedSalary | \n",
    "| :---        |    :----:   |          ---: | :---        |    :----:   |          ---: | :---        |    :----:   |          ---: | :---        |    :----:   |          ---: |\n",
    "| 55443323 | Nguyen |603|France|Female|76|20|123456.78|5|1|1|55000.00|\n",
    "\n",
    "\n",
    "* Load your trained model `model-ann-11-5-4-1-xx` from the file, and have it predict the test sample above. Please don't forget to preprocess this test samples so that it is compliant with the input and model requirements.\n",
    "* Please report whether it predicts a 0 or 1 for the `Exited` target. Can you comment on this data sample whether your model captured the pattern in the population?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define activation functions\n",
    "class ReLU:\n",
    "    def __call__(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "class Sigmoid:\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return self.__call__(x) * (1 - self.__call__(x))\n",
    "\n",
    "# Define layer class\n",
    "class Dense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.activation = activation()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        self.output = np.dot(x, self.weights) + self.bias\n",
    "        return self.activation(self.output)\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.input.T, grad_output)\n",
    "        grad_bias = np.sum(grad_output, axis=0, keepdims=True)\n",
    "\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.bias -= learning_rate * grad_bias\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "# Define binary crossentropy loss\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Define neural network class\n",
    "class ANN:\n",
    "    def __init__(self):\n",
    "        self.layer1 = Dense(11, 5, ReLU)\n",
    "        self.layer2 = Dense(5, 4, ReLU)\n",
    "        self.output_layer = Dense(4, 1, Sigmoid)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_output = self.output_layer.backward(grad_output, learning_rate)\n",
    "        grad_output = self.layer2.backward(grad_output, learning_rate)\n",
    "        grad_output = self.layer1.backward(grad_output, learning_rate)\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        np.savez(file_path, layer1_weights=self.layer1.weights, layer1_bias=self.layer1.bias,\n",
    "                 layer2_weights=self.layer2.weights, layer2_bias=self.layer2.bias,\n",
    "                 output_weights=self.output_layer.weights, output_bias=self.output_layer.bias)\n",
    "\n",
    "# Training loop\n",
    "def train(X_train, y_train, epochs, learning_rate):\n",
    "    model = ANN()\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        y_pred = model(X_train)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = binary_crossentropy(y_train, y_pred)\n",
    "        \n",
    "        # Backward pass\n",
    "        grad_output = y_pred - y_train\n",
    "        model.backward(grad_output, learning_rate)\n",
    "        \n",
    "        # Print loss\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Loss {loss}\")\n",
    "\n",
    "    # Save model after training\n",
    "    model.save_model(\"saved_models/model-ann-11-5-4-1-mc.h5\")\n",
    "\n",
    "\n",
    "    train(X_train_scaled, y_train, epochs=25, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ANN-1_layers",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 5)                 60        \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4)                 24        \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89 (356.00 Byte)\n",
      "Trainable params: 89 (356.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def create_model():\n",
    "   \n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, input_dim=11, activation='relu'))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "215/215 [==============================] - 1s 1ms/step - loss: 0.8333 - accuracy: 0.4080\n",
      "Epoch 2/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.6440 - accuracy: 0.7320\n",
      "Epoch 3/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.5966 - accuracy: 0.7871\n",
      "Epoch 4/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.5682 - accuracy: 0.7919\n",
      "Epoch 5/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.5459 - accuracy: 0.7928\n",
      "Epoch 6/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.5253 - accuracy: 0.7963\n",
      "Epoch 7/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.5020 - accuracy: 0.8110\n",
      "Epoch 8/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.4802 - accuracy: 0.8210\n",
      "Epoch 9/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.4615 - accuracy: 0.8307\n",
      "Epoch 10/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.4462 - accuracy: 0.8369\n",
      "Epoch 11/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.4325 - accuracy: 0.8400\n",
      "Epoch 12/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.4215 - accuracy: 0.8429\n",
      "Epoch 13/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.4111 - accuracy: 0.8468\n",
      "Epoch 14/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.4027 - accuracy: 0.8474\n",
      "Epoch 15/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3943 - accuracy: 0.8489\n",
      "Epoch 16/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3866 - accuracy: 0.8518\n",
      "Epoch 17/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3797 - accuracy: 0.8518\n",
      "Epoch 18/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3747 - accuracy: 0.8556\n",
      "Epoch 19/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8576\n",
      "Epoch 20/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3660 - accuracy: 0.8573\n",
      "Epoch 21/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3632 - accuracy: 0.8582\n",
      "Epoch 22/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3602 - accuracy: 0.8595\n",
      "Epoch 23/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3575 - accuracy: 0.8586\n",
      "Epoch 24/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3553 - accuracy: 0.8600\n",
      "Epoch 25/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3535 - accuracy: 0.8595\n",
      " 1/54 [..............................] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/personal/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 1ms/step\n",
      "Model Performance on Test Set:\n",
      "Accuracy: 0.8456\n",
      "Precision: 0.7175\n",
      "Recall: 0.4420\n",
      "F1 Score: 0.5470\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=25,verbose=1)\n",
    "model_name = \"saved_models/model-ann-11-5-4-1-tf.h5\"\n",
    "model.save(model_name)\n",
    "\n",
    "model_loaded = tf.keras.models.load_model(model_name)\n",
    "y_pred = model_loaded.predict(X_test_scaled)\n",
    "y_pred_binary = (y_pred > 0.5).astype(float) \n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Model Performance on Test Set:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step\n",
      "Predicted Exit: 0\n",
      "True Exit: 0\n",
      "Model predicts correctly!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Preprocess the test sample\n",
    "test_sample = {\n",
    "    'CreditScore': [709],\n",
    "    'Age': [30],\n",
    "    'Tenure': [9],\n",
    "    'Balance': [115479.48],\n",
    "    'NumOfProducts': [2],\n",
    "    'HasCrCard': [1],\n",
    "    'IsActiveMember': [1],\n",
    "    'EstimatedSalary': [134732.99],\n",
    "    'Geography_Germany': [1.0],\n",
    "    'Geography_Spain': [0.0],\n",
    "    'Gender_Male': [1.0]\n",
    "}\n",
    "\n",
    "# Convert the test sample to a DataFrame\n",
    "test_df = pd.DataFrame(test_sample)\n",
    "\n",
    "# Scale the test sample\n",
    "test_scaled = scaler.fit_transform(test_df)\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = tf.keras.models.load_model(\"saved_models/model-ann-11-5-4-1-tf.h5\")\n",
    "\n",
    "# Predict the outcome for the test sample\n",
    "prediction = loaded_model.predict(test_scaled)\n",
    "\n",
    "# Convert the prediction to binary (0 or 1)\n",
    "predicted_exit = np.round(prediction)[0][0]\n",
    "\n",
    "# Ground true Exited value for the test sample\n",
    "true_exit = 0\n",
    "\n",
    "# Report the prediction\n",
    "print(\"Predicted Exit:\", int(predicted_exit))\n",
    "print(\"True Exit:\", true_exit)\n",
    "\n",
    "# Comment on the prediction\n",
    "if predicted_exit == true_exit:\n",
    "    print(\"Model predicts correctly!\")\n",
    "else:\n",
    "    print(\"Model makes a mistake.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n",
      "Predicted Exit: 0\n",
      "True Exit: 0\n",
      "Model predicts correctly!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Preprocess the test sample\n",
    "test_sample1 = {\n",
    "    'CreditScore': [603],\n",
    "    'Age': [76],\n",
    "    'Tenure': [20],\n",
    "    'Balance': [123456.78],\n",
    "    'NumOfProducts': [5],\n",
    "    'HasCrCard': [1],\n",
    "    'IsActiveMember': [1],\n",
    "    'EstimatedSalary': [55000.00],\n",
    "    'Geography_Germany': [0.0],\n",
    "    'Geography_Spain': [0.0],\n",
    "    'Gender_Male': [0.0]\n",
    "}\n",
    "\n",
    "# Convert the test sample to a DataFrame\n",
    "test_df = pd.DataFrame(test_sample1)\n",
    "\n",
    "# Scale the test sample\n",
    "test_scaled = scaler.fit_transform(test_df)\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = tf.keras.models.load_model(\"saved_models/model-ann-11-5-4-1-tf.h5\")\n",
    "\n",
    "# Predict the outcome for the test sample\n",
    "prediction = loaded_model.predict(test_scaled)\n",
    "\n",
    "# Convert the prediction to binary (0 or 1)\n",
    "predicted_exit = np.round(prediction)[0][0]\n",
    "\n",
    "# Ground true Exited value for the test sample\n",
    "true_exit = 0\n",
    "\n",
    "# Report the prediction\n",
    "print(\"Predicted Exit:\", int(predicted_exit))\n",
    "print(\"True Exit:\", true_exit)\n",
    "\n",
    "# Comment on the prediction\n",
    "if predicted_exit == true_exit:\n",
    "    print(\"Model predicts correctly!\")\n",
    "else:\n",
    "    print(\"Model makes a mistake.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: (10 points)\n",
    "\n",
    "* Repeat Task 3 with the following new architecture of the neural network:\n",
    "\n",
    "![Task 4 ANN architecture](figures/nn-2.png)\n",
    "\n",
    "* Input layer will still have 11 units as the dimension of training set (i.e, number of columns = 11).\n",
    "* Hidden-layer-1: 8 neurons, with relu activation\n",
    "* Hidden-layer-2: 8 neurons, with relu activation,\n",
    "* Hidden-layer-3: 8 neurons, with relu activation,\n",
    "* Output-layer: 1 neuron with sigmoid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define activation functions\n",
    "class ReLU:\n",
    "    def __call__(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "class Sigmoid:\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return self.__call__(x) * (1 - self.__call__(x))\n",
    "\n",
    "# Define layer class\n",
    "class Dense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.activation = activation()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        self.output = np.dot(x, self.weights) + self.bias\n",
    "        return self.activation(self.output)\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.input.T, grad_output)\n",
    "        grad_bias = np.sum(grad_output, axis=0, keepdims=True)\n",
    "\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.bias -= learning_rate * grad_bias\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "# Define binary crossentropy loss\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Define neural network class\n",
    "class ANN:\n",
    "    def __init__(self):\n",
    "        self.layer1 = Dense(11, 8, ReLU)\n",
    "        self.layer2 = Dense(8, 8, ReLU)\n",
    "        self.layer3 = Dense(8, 8, ReLU)\n",
    "        self.output_layer = Dense(8, 1, Sigmoid)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_output = self.output_layer.backward(grad_output, learning_rate)\n",
    "        grad_output = self.layer2.backward(grad_output, learning_rate)\n",
    "        grad_output = self.layer1.backward(grad_output, learning_rate)\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        np.savez(file_path, layer1_weights=self.layer1.weights, layer1_bias=self.layer1.bias,\n",
    "                 layer2_weights=self.layer2.weights, layer2_bias=self.layer2.bias,\n",
    "                 output_weights=self.output_layer.weights, output_bias=self.output_layer.bias)\n",
    "\n",
    "# Training loop\n",
    "def train(X_train, y_train, epochs, learning_rate):\n",
    "    model = ANN()\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        y_pred = model(X_train)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = binary_crossentropy(y_train, y_pred)\n",
    "        \n",
    "        # Backward pass\n",
    "        grad_output = y_pred - y_train\n",
    "        model.backward(grad_output, learning_rate)\n",
    "        \n",
    "        # Print loss\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Loss {loss}\")\n",
    "\n",
    "    # Save model after training\n",
    "    model.save_model(\"saved_models/model-ann-11-5-4-1-mc.h5\")\n",
    "\n",
    "\n",
    "    train(X_train_scaled, y_train, epochs=25, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_17 (Dense)            (None, 8)                 96        \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 249 (996.00 Byte)\n",
      "Trainable params: 249 (996.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def create_model():\n",
    "   \n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=11, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "215/215 [==============================] - 1s 1ms/step - loss: 0.5506 - accuracy: 0.7673\n",
      "Epoch 2/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.7926\n",
      "Epoch 3/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.4409 - accuracy: 0.7944\n",
      "Epoch 4/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.4265 - accuracy: 0.8041\n",
      "Epoch 5/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.4135 - accuracy: 0.8197\n",
      "Epoch 6/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.4004 - accuracy: 0.8269\n",
      "Epoch 7/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3879 - accuracy: 0.8342\n",
      "Epoch 8/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3768 - accuracy: 0.8419\n",
      "Epoch 9/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8473\n",
      "Epoch 10/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3620 - accuracy: 0.8521\n",
      "Epoch 11/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3559 - accuracy: 0.8560\n",
      "Epoch 12/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3522 - accuracy: 0.8600\n",
      "Epoch 13/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3489 - accuracy: 0.8607\n",
      "Epoch 14/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3459 - accuracy: 0.8600\n",
      "Epoch 15/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3440 - accuracy: 0.8600\n",
      "Epoch 16/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3428 - accuracy: 0.8594\n",
      "Epoch 17/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3407 - accuracy: 0.8613\n",
      "Epoch 18/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3404 - accuracy: 0.8626\n",
      "Epoch 19/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3390 - accuracy: 0.8610\n",
      "Epoch 20/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3389 - accuracy: 0.8618\n",
      "Epoch 21/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3377 - accuracy: 0.8617\n",
      "Epoch 22/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3370 - accuracy: 0.8621\n",
      "Epoch 23/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3369 - accuracy: 0.8617\n",
      "Epoch 24/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3353 - accuracy: 0.8636\n",
      "Epoch 25/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.8637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/personal/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 1ms/step\n",
      "Model Performance on Test Set:\n",
      "Accuracy: 0.8415\n",
      "Precision: 0.6744\n",
      "Recall: 0.4807\n",
      "F1 Score: 0.5613\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=25,verbose=1)\n",
    "model_name = \"saved_models/model-ann-11-5-4-1-tf.h5\"\n",
    "model.save(model_name)\n",
    "\n",
    "model_loaded = tf.keras.models.load_model(model_name)\n",
    "y_pred = model_loaded.predict(X_test_scaled)\n",
    "y_pred_binary = (y_pred > 0.5).astype(float) \n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Model Performance on Test Set:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "Predicted Exit: 0\n",
      "True Exit: 0\n",
      "Model predicts correctly!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Preprocess the test sample\n",
    "test_sample = {\n",
    "    'CreditScore': [709],\n",
    "    'Age': [30],\n",
    "    'Tenure': [9],\n",
    "    'Balance': [115479.48],\n",
    "    'NumOfProducts': [2],\n",
    "    'HasCrCard': [1],\n",
    "    'IsActiveMember': [1],\n",
    "    'EstimatedSalary': [134732.99],\n",
    "    'Geography_Germany': [1.0],\n",
    "    'Geography_Spain': [0.0],\n",
    "    'Gender_Male': [1.0]\n",
    "}\n",
    "\n",
    "# Convert the test sample to a DataFrame\n",
    "test_df = pd.DataFrame(test_sample)\n",
    "\n",
    "# Scale the test sample\n",
    "test_scaled = scaler.fit_transform(test_df)\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = tf.keras.models.load_model(\"saved_models/model-ann-11-5-4-1-tf.h5\")\n",
    "\n",
    "# Predict the outcome for the test sample\n",
    "prediction = loaded_model.predict(test_scaled)\n",
    "\n",
    "# Convert the prediction to binary (0 or 1)\n",
    "predicted_exit = np.round(prediction)[0][0]\n",
    "\n",
    "# Ground true Exited value for the test sample\n",
    "true_exit = 0\n",
    "\n",
    "# Report the prediction\n",
    "print(\"Predicted Exit:\", int(predicted_exit))\n",
    "print(\"True Exit:\", true_exit)\n",
    "\n",
    "# Comment on the prediction\n",
    "if predicted_exit == true_exit:\n",
    "    print(\"Model predicts correctly!\")\n",
    "else:\n",
    "    print(\"Model makes a mistake.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 58 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14eeae200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Predicted Exit: 0\n",
      "True Exit: 0\n",
      "Model predicts correctly!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Preprocess the test sample\n",
    "test_sample1 = {\n",
    "    'CreditScore': [603],\n",
    "    'Age': [76],\n",
    "    'Tenure': [20],\n",
    "    'Balance': [123456.78],\n",
    "    'NumOfProducts': [5],\n",
    "    'HasCrCard': [1],\n",
    "    'IsActiveMember': [1],\n",
    "    'EstimatedSalary': [55000.00],\n",
    "    'Geography_Germany': [0.0],\n",
    "    'Geography_Spain': [0.0],\n",
    "    'Gender_Male': [0.0]\n",
    "}\n",
    "\n",
    "# Convert the test sample to a DataFrame\n",
    "test_df = pd.DataFrame(test_sample1)\n",
    "\n",
    "# Scale the test sample\n",
    "test_scaled = scaler.fit_transform(test_df)\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = tf.keras.models.load_model(\"saved_models/model-ann-11-5-4-1-tf.h5\")\n",
    "\n",
    "# Predict the outcome for the test sample\n",
    "prediction = loaded_model.predict(test_scaled)\n",
    "\n",
    "# Convert the prediction to binary (0 or 1)\n",
    "predicted_exit = np.round(prediction)[0][0]\n",
    "\n",
    "# Ground true Exited value for the test sample\n",
    "true_exit = 0\n",
    "\n",
    "# Report the prediction\n",
    "print(\"Predicted Exit:\", int(predicted_exit))\n",
    "print(\"True Exit:\", true_exit)\n",
    "\n",
    "# Comment on the prediction\n",
    "if predicted_exit == true_exit:\n",
    "    print(\"Model predicts correctly!\")\n",
    "else:\n",
    "    print(\"Model makes a mistake.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: (10 points)\n",
    "\n",
    "* Repeat Task 3 with the following new architecture of the neural network:\n",
    "\n",
    "![Task 5 ANN architecture](figures/nn-3.png)\n",
    "\n",
    "* Input layer will still have 11 units as the dimension of training set (i.e, number of columns = 11).\n",
    "* Hidden-layer-1: 8 neurons, with relu activation\n",
    "* Hidden-layer-2: 4 neurons, with relu activation,\n",
    "* Hidden-layer-3: 2 neurons, with relu activation,\n",
    "* Output-layer: 1 neuron with sigmoid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define activation functions\n",
    "class ReLU:\n",
    "    def __call__(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "class Sigmoid:\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return self.__call__(x) * (1 - self.__call__(x))\n",
    "\n",
    "# Define layer class\n",
    "class Dense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.activation = activation()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        self.output = np.dot(x, self.weights) + self.bias\n",
    "        return self.activation(self.output)\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.input.T, grad_output)\n",
    "        grad_bias = np.sum(grad_output, axis=0, keepdims=True)\n",
    "\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.bias -= learning_rate * grad_bias\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "# Define binary crossentropy loss\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Define neural network class\n",
    "class ANN:\n",
    "    def __init__(self):\n",
    "        self.layer1 = Dense(11, 8, ReLU)\n",
    "        self.layer2 = Dense(8, 4, ReLU)\n",
    "        self.layer3 = Dense(4, 2, ReLU)\n",
    "        self.output_layer = Dense(2, 1, Sigmoid)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_output = self.output_layer.backward(grad_output, learning_rate)\n",
    "        grad_output = self.layer2.backward(grad_output, learning_rate)\n",
    "        grad_output = self.layer1.backward(grad_output, learning_rate)\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        np.savez(file_path, layer1_weights=self.layer1.weights, layer1_bias=self.layer1.bias,\n",
    "                 layer2_weights=self.layer2.weights, layer2_bias=self.layer2.bias,\n",
    "                 output_weights=self.output_layer.weights, output_bias=self.output_layer.bias)\n",
    "\n",
    "# Training loop\n",
    "def train(X_train, y_train, epochs, learning_rate):\n",
    "    model = ANN()\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        y_pred = model(X_train)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = binary_crossentropy(y_train, y_pred)\n",
    "        \n",
    "        # Backward pass\n",
    "        grad_output = y_pred - y_train\n",
    "        model.backward(grad_output, learning_rate)\n",
    "        \n",
    "        # Print loss\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Loss {loss}\")\n",
    "\n",
    "    # Save model after training\n",
    "    model.save_model(\"saved_models/model-ann-11-5-4-1-mc.h5\")\n",
    "\n",
    "\n",
    "    train(X_train_scaled, y_train, epochs=25, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_21 (Dense)            (None, 8)                 96        \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 145 (580.00 Byte)\n",
      "Trainable params: 145 (580.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def create_model():\n",
    "   \n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=11, activation='relu'))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(2, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "215/215 [==============================] - 1s 1ms/step - loss: 0.5942 - accuracy: 0.7378\n",
      "Epoch 2/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.4659 - accuracy: 0.8021\n",
      "Epoch 3/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.4295 - accuracy: 0.8171\n",
      "Epoch 4/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.4112 - accuracy: 0.8231\n",
      "Epoch 5/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3977 - accuracy: 0.8307\n",
      "Epoch 6/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3874 - accuracy: 0.8365\n",
      "Epoch 7/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3775 - accuracy: 0.8470\n",
      "Epoch 8/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3697 - accuracy: 0.8490\n",
      "Epoch 9/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3643 - accuracy: 0.8549\n",
      "Epoch 10/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3602 - accuracy: 0.8563\n",
      "Epoch 11/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3569 - accuracy: 0.8573\n",
      "Epoch 12/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3550 - accuracy: 0.8572\n",
      "Epoch 13/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3527 - accuracy: 0.8585\n",
      "Epoch 14/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.8598\n",
      "Epoch 15/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3492 - accuracy: 0.8604\n",
      "Epoch 16/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3478 - accuracy: 0.8598\n",
      "Epoch 17/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3459 - accuracy: 0.8623\n",
      "Epoch 18/25\n",
      "215/215 [==============================] - 0s 2ms/step - loss: 0.3458 - accuracy: 0.8623\n",
      "Epoch 19/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3445 - accuracy: 0.8614\n",
      "Epoch 20/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3437 - accuracy: 0.8626\n",
      "Epoch 21/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3430 - accuracy: 0.8627\n",
      "Epoch 22/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3415 - accuracy: 0.8624\n",
      "Epoch 23/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3411 - accuracy: 0.8626\n",
      "Epoch 24/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3409 - accuracy: 0.8632\n",
      "Epoch 25/25\n",
      "215/215 [==============================] - 0s 1ms/step - loss: 0.3400 - accuracy: 0.8646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/personal/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 1ms/step\n",
      "Model Performance on Test Set:\n",
      "Accuracy: 0.8479\n",
      "Precision: 0.7205\n",
      "Recall: 0.4558\n",
      "F1 Score: 0.5584\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=25,verbose=1)\n",
    "model_name = \"saved_models/model-ann-11-5-4-1-tf.h5\"\n",
    "model.save(model_name)\n",
    "\n",
    "model_loaded = tf.keras.models.load_model(model_name)\n",
    "y_pred = model_loaded.predict(X_test_scaled)\n",
    "y_pred_binary = (y_pred > 0.5).astype(float) \n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Model Performance on Test Set:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 59 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14c555bc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Predicted Exit: 0\n",
      "True Exit: 0\n",
      "Model predicts correctly!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Preprocess the test sample\n",
    "test_sample = {\n",
    "    'CreditScore': [709],\n",
    "    'Age': [30],\n",
    "    'Tenure': [9],\n",
    "    'Balance': [115479.48],\n",
    "    'NumOfProducts': [2],\n",
    "    'HasCrCard': [1],\n",
    "    'IsActiveMember': [1],\n",
    "    'EstimatedSalary': [134732.99],\n",
    "    'Geography_Germany': [1.0],\n",
    "    'Geography_Spain': [0.0],\n",
    "    'Gender_Male': [1.0]\n",
    "}\n",
    "\n",
    "# Convert the test sample to a DataFrame\n",
    "test_df = pd.DataFrame(test_sample)\n",
    "\n",
    "# Scale the test sample\n",
    "test_scaled = scaler.fit_transform(test_df)\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = tf.keras.models.load_model(\"saved_models/model-ann-11-5-4-1-tf.h5\")\n",
    "\n",
    "# Predict the outcome for the test sample\n",
    "prediction = loaded_model.predict(test_scaled)\n",
    "\n",
    "# Convert the prediction to binary (0 or 1)\n",
    "predicted_exit = np.round(prediction)[0][0]\n",
    "\n",
    "# Ground true Exited value for the test sample\n",
    "true_exit = 0\n",
    "\n",
    "# Report the prediction\n",
    "print(\"Predicted Exit:\", int(predicted_exit))\n",
    "print(\"True Exit:\", true_exit)\n",
    "\n",
    "# Comment on the prediction\n",
    "if predicted_exit == true_exit:\n",
    "    print(\"Model predicts correctly!\")\n",
    "else:\n",
    "    print(\"Model makes a mistake.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "Predicted Exit: 0\n",
      "True Exit: 0\n",
      "Model predicts correctly!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Preprocess the test sample\n",
    "test_sample1 = {\n",
    "    'CreditScore': [603],\n",
    "    'Age': [76],\n",
    "    'Tenure': [20],\n",
    "    'Balance': [123456.78],\n",
    "    'NumOfProducts': [5],\n",
    "    'HasCrCard': [1],\n",
    "    'IsActiveMember': [1],\n",
    "    'EstimatedSalary': [55000.00],\n",
    "    'Geography_Germany': [0.0],\n",
    "    'Geography_Spain': [0.0],\n",
    "    'Gender_Male': [0.0]\n",
    "}\n",
    "\n",
    "# Convert the test sample to a DataFrame\n",
    "test_df = pd.DataFrame(test_sample1)\n",
    "\n",
    "# Scale the test sample\n",
    "test_scaled = scaler.fit_transform(test_df)\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = tf.keras.models.load_model(\"saved_models/model-ann-11-5-4-1-tf.h5\")\n",
    "\n",
    "# Predict the outcome for the test sample\n",
    "prediction = loaded_model.predict(test_scaled)\n",
    "\n",
    "# Convert the prediction to binary (0 or 1)\n",
    "predicted_exit = np.round(prediction)[0][0]\n",
    "\n",
    "# Ground true Exited value for the test sample\n",
    "true_exit = 0\n",
    "\n",
    "# Report the prediction\n",
    "print(\"Predicted Exit:\", int(predicted_exit))\n",
    "print(\"True Exit:\", true_exit)\n",
    "\n",
    "# Comment on the prediction\n",
    "if predicted_exit == true_exit:\n",
    "    print(\"Model predicts correctly!\")\n",
    "else:\n",
    "    print(\"Model makes a mistake.\")\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
