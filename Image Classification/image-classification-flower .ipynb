{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9266774,"sourceType":"datasetVersion","datasetId":5607846},{"sourceId":9270296,"sourceType":"datasetVersion","datasetId":5609984},{"sourceId":9284366,"sourceType":"datasetVersion","datasetId":5619970}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Conv2D, Flatten, MaxPool2D, BatchNormalization\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.applications import VGG16\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T21:18:58.506031Z","iopub.execute_input":"2024-09-02T21:18:58.506715Z","iopub.status.idle":"2024-09-02T21:18:58.513144Z","shell.execute_reply.started":"2024-09-02T21:18:58.506674Z","shell.execute_reply":"2024-09-02T21:18:58.512216Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"img_height, img_width=(224,224)\nbatch_size=32\n\ntrain_data=r\"/kaggle/input/flower-recognition-dataset/flowers1/train\"\nval_data=r\"/kaggle/input/flower-recognition-dataset/flowers1/val\"\ntest_data=r\"/kaggle/input/flower-recognition-dataset/flowers1/test\"","metadata":{"execution":{"iopub.status.busy":"2024-09-02T21:19:01.039782Z","iopub.execute_input":"2024-09-02T21:19:01.040158Z","iopub.status.idle":"2024-09-02T21:19:01.045110Z","shell.execute_reply.started":"2024-09-02T21:19:01.040123Z","shell.execute_reply":"2024-09-02T21:19:01.044026Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_datagen = image.ImageDataGenerator(\n     preprocessing_function=tf.keras.applications.vgg16.preprocess_input,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data,\n    target_size=(img_height, img_width),\n    batch_size=16,\n    class_mode='categorical'\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:58:02.153846Z","iopub.execute_input":"2024-09-02T18:58:02.154325Z","iopub.status.idle":"2024-09-02T18:58:02.262345Z","shell.execute_reply.started":"2024-09-02T18:58:02.154288Z","shell.execute_reply":"2024-09-02T18:58:02.261458Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Found 2588 images belonging to 5 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_datagen = image.ImageDataGenerator(\n     preprocessing_function=preprocess_input,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data,\n    target_size=(img_height, img_width),\n    batch_size=16,\n    class_mode='categorical'\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T21:19:05.274295Z","iopub.execute_input":"2024-09-02T21:19:05.275159Z","iopub.status.idle":"2024-09-02T21:19:06.114262Z","shell.execute_reply.started":"2024-09-02T21:19:05.275118Z","shell.execute_reply":"2024-09-02T21:19:06.113481Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Found 2588 images belonging to 5 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"val_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\n\nvalidation_set = val_datagen.flow_from_directory(\n    val_data,\n    target_size=(img_height,img_width),\n    batch_size=16,\n    class_mode='categorical'\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:58:05.259554Z","iopub.execute_input":"2024-09-02T18:58:05.259941Z","iopub.status.idle":"2024-09-02T18:58:05.301902Z","shell.execute_reply.started":"2024-09-02T18:58:05.259903Z","shell.execute_reply":"2024-09-02T18:58:05.301006Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Found 860 images belonging to 5 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"val_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preprocess_input)\n\nvalidation_set = val_datagen.flow_from_directory(\n    val_data,\n    target_size=(img_height,img_width),\n    batch_size=16,\n    class_mode='categorical'\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T21:19:09.801548Z","iopub.execute_input":"2024-09-02T21:19:09.802281Z","iopub.status.idle":"2024-09-02T21:19:10.107626Z","shell.execute_reply.started":"2024-09-02T21:19:09.802242Z","shell.execute_reply":"2024-09-02T21:19:10.106851Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Found 860 images belonging to 5 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"x, y = next(test_set)\nprint(x.shape)\nprint(y.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:58:10.601449Z","iopub.execute_input":"2024-09-02T18:58:10.602231Z","iopub.status.idle":"2024-09-02T18:58:10.844214Z","shell.execute_reply.started":"2024-09-02T18:58:10.602194Z","shell.execute_reply":"2024-09-02T18:58:10.843188Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"(16, 224, 224, 3)\n(16, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n\n# Freeze the layers except the last 4 layers\nfor layer in base_model.layers[:-4]:\n    layer.trainable = False\n\n# Create the custom layers atop the base model\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(train_generator.num_classes, activation='softmax')(x)\n\n# Connecting base model with new layers\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.0001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:58:14.085335Z","iopub.execute_input":"2024-09-02T18:58:14.085720Z","iopub.status.idle":"2024-09-02T18:58:14.905774Z","shell.execute_reply.started":"2024-09-02T18:58:14.085685Z","shell.execute_reply":"2024-09-02T18:58:14.904958Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\ncheckpoint = ModelCheckpoint(\"vgg16_flowers.keras\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto')\nearly = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=1, mode='auto')\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001, verbose=1)\n\nhistory = model.fit(\n    train_generator,\n    epochs=50,\n    validation_data=validation_set,\n    callbacks=[checkpoint, early, reduce_lr]\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T19:00:12.913473Z","iopub.execute_input":"2024-09-02T19:00:12.914291Z","iopub.status.idle":"2024-09-02T19:15:06.142055Z","shell.execute_reply.started":"2024-09-02T19:00:12.914249Z","shell.execute_reply":"2024-09-02T19:15:06.141151Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - accuracy: 0.4117 - loss: 1.7790\nEpoch 1: val_accuracy improved from -inf to 0.37209, saving model to vgg16_flowers.keras\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 329ms/step - accuracy: 0.4127 - loss: 1.7750 - val_accuracy: 0.3721 - val_loss: 1.5682 - learning_rate: 1.0000e-04\nEpoch 2/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.7936 - loss: 0.6157\nEpoch 2: val_accuracy improved from 0.37209 to 0.50000, saving model to vgg16_flowers.keras\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 251ms/step - accuracy: 0.7938 - loss: 0.6153 - val_accuracy: 0.5000 - val_loss: 1.5109 - learning_rate: 1.0000e-04\nEpoch 3/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8476 - loss: 0.4467\nEpoch 3: val_accuracy did not improve from 0.50000\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 243ms/step - accuracy: 0.8476 - loss: 0.4465 - val_accuracy: 0.4686 - val_loss: 1.4868 - learning_rate: 1.0000e-04\nEpoch 4/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.8596 - loss: 0.3842\nEpoch 4: val_accuracy did not improve from 0.50000\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 246ms/step - accuracy: 0.8596 - loss: 0.3843 - val_accuracy: 0.4140 - val_loss: 1.5034 - learning_rate: 1.0000e-04\nEpoch 5/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.9070 - loss: 0.2893\nEpoch 5: val_accuracy did not improve from 0.50000\n\nEpoch 5: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 246ms/step - accuracy: 0.9069 - loss: 0.2894 - val_accuracy: 0.4686 - val_loss: 1.4951 - learning_rate: 1.0000e-04\nEpoch 6/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.9237 - loss: 0.2195\nEpoch 6: val_accuracy improved from 0.50000 to 0.51163, saving model to vgg16_flowers.keras\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 246ms/step - accuracy: 0.9238 - loss: 0.2194 - val_accuracy: 0.5116 - val_loss: 1.4678 - learning_rate: 2.0000e-05\nEpoch 7/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.9390 - loss: 0.1653\nEpoch 7: val_accuracy improved from 0.51163 to 0.51744, saving model to vgg16_flowers.keras\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 249ms/step - accuracy: 0.9390 - loss: 0.1653 - val_accuracy: 0.5174 - val_loss: 1.4463 - learning_rate: 2.0000e-05\nEpoch 8/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.9594 - loss: 0.1161\nEpoch 8: val_accuracy improved from 0.51744 to 0.54302, saving model to vgg16_flowers.keras\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 251ms/step - accuracy: 0.9594 - loss: 0.1162 - val_accuracy: 0.5430 - val_loss: 1.4348 - learning_rate: 2.0000e-05\nEpoch 9/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.9537 - loss: 0.1321\nEpoch 9: val_accuracy did not improve from 0.54302\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 242ms/step - accuracy: 0.9537 - loss: 0.1321 - val_accuracy: 0.5360 - val_loss: 1.4406 - learning_rate: 2.0000e-05\nEpoch 10/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.9591 - loss: 0.1203\nEpoch 10: val_accuracy did not improve from 0.54302\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 242ms/step - accuracy: 0.9591 - loss: 0.1202 - val_accuracy: 0.5419 - val_loss: 1.4177 - learning_rate: 2.0000e-05\nEpoch 11/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.9577 - loss: 0.1108\nEpoch 11: val_accuracy improved from 0.54302 to 0.57907, saving model to vgg16_flowers.keras\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 246ms/step - accuracy: 0.9577 - loss: 0.1108 - val_accuracy: 0.5791 - val_loss: 1.4235 - learning_rate: 2.0000e-05\nEpoch 12/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.9671 - loss: 0.1002\nEpoch 12: val_accuracy did not improve from 0.57907\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 243ms/step - accuracy: 0.9672 - loss: 0.1001 - val_accuracy: 0.5570 - val_loss: 1.4103 - learning_rate: 2.0000e-05\nEpoch 13/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.9685 - loss: 0.0959\nEpoch 13: val_accuracy did not improve from 0.57907\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 242ms/step - accuracy: 0.9685 - loss: 0.0960 - val_accuracy: 0.5453 - val_loss: 1.4154 - learning_rate: 2.0000e-05\nEpoch 14/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.9782 - loss: 0.0694\nEpoch 14: val_accuracy did not improve from 0.57907\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 243ms/step - accuracy: 0.9782 - loss: 0.0694 - val_accuracy: 0.5430 - val_loss: 1.4002 - learning_rate: 2.0000e-05\nEpoch 15/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.9778 - loss: 0.0783\nEpoch 15: val_accuracy did not improve from 0.57907\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 241ms/step - accuracy: 0.9778 - loss: 0.0783 - val_accuracy: 0.5419 - val_loss: 1.3905 - learning_rate: 2.0000e-05\nEpoch 16/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.9760 - loss: 0.0622\nEpoch 16: val_accuracy did not improve from 0.57907\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 243ms/step - accuracy: 0.9760 - loss: 0.0622 - val_accuracy: 0.5500 - val_loss: 1.4002 - learning_rate: 2.0000e-05\nEpoch 17/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.9840 - loss: 0.0539\nEpoch 17: val_accuracy did not improve from 0.57907\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 246ms/step - accuracy: 0.9840 - loss: 0.0539 - val_accuracy: 0.5686 - val_loss: 1.3895 - learning_rate: 2.0000e-05\nEpoch 18/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.9820 - loss: 0.0537\nEpoch 18: val_accuracy did not improve from 0.57907\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 242ms/step - accuracy: 0.9820 - loss: 0.0538 - val_accuracy: 0.5512 - val_loss: 1.3845 - learning_rate: 2.0000e-05\nEpoch 19/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.9926 - loss: 0.0318\nEpoch 19: val_accuracy did not improve from 0.57907\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 241ms/step - accuracy: 0.9926 - loss: 0.0319 - val_accuracy: 0.5314 - val_loss: 1.3991 - learning_rate: 2.0000e-05\nEpoch 20/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.9872 - loss: 0.0457\nEpoch 20: val_accuracy did not improve from 0.57907\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 241ms/step - accuracy: 0.9872 - loss: 0.0456 - val_accuracy: 0.5500 - val_loss: 1.3586 - learning_rate: 2.0000e-05\nEpoch 21/50\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.9782 - loss: 0.0634\nEpoch 21: val_accuracy did not improve from 0.57907\n\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 243ms/step - accuracy: 0.9783 - loss: 0.0634 - val_accuracy: 0.5558 - val_loss: 1.3687 - learning_rate: 2.0000e-05\nEpoch 21: early stopping\n","output_type":"stream"}]},{"cell_type":"code","source":"#test_data_dir = \"/path/to/test\"  # Set the path to your test directory\n\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.vgg16.preprocess_input\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_data,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False  # Important for correct label ordering in confusion matrix\n)\n\n# Evaluate the model on the test data\nscores = model.evaluate(test_generator)\nprint(f\"Test Loss: {scores[0]}, Test Accuracy: {scores[1]}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T19:19:18.306974Z","iopub.execute_input":"2024-09-02T19:19:18.307856Z","iopub.status.idle":"2024-09-02T19:19:45.834431Z","shell.execute_reply.started":"2024-09-02T19:19:18.307817Z","shell.execute_reply":"2024-09-02T19:19:45.833503Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Found 774 images belonging to 5 classes.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 503ms/step - accuracy: 0.9118 - loss: 0.5992\nTest Loss: 0.5103763937950134, Test Accuracy: 0.9147287011146545\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\n\n# Predict the output\ntest_generator.reset()\npred = model.predict(test_generator, steps=len(test_generator), verbose=1)\npredicted_class_indices = np.argmax(pred, axis=1)\n\n# True labels\nlabels = (test_generator.class_indices)\nlabels = dict((v, k) for k, v in labels.items())\ntrue_class_indices = test_generator.classes\n\n# Confusion Matrix and Classification Report\nconf_matrix = confusion_matrix(true_class_indices, predicted_class_indices)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\nreport = classification_report(true_class_indices, predicted_class_indices, target_names=list(labels.values()))\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T19:19:59.660584Z","iopub.execute_input":"2024-09-02T19:19:59.660971Z","iopub.status.idle":"2024-09-02T19:20:03.826257Z","shell.execute_reply.started":"2024-09-02T19:19:59.660933Z","shell.execute_reply":"2024-09-02T19:20:03.825259Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step\nConfusion Matrix:\n[[122   4   1   3   4]\n [  2 181   0   4   5]\n [  0   1 124   1  18]\n [  0   4   0 121   4]\n [  1   1  11   2 160]]\nClassification Report:\n              precision    recall  f1-score   support\n\n       daisy       0.98      0.91      0.94       134\n   dandelion       0.95      0.94      0.95       192\n        rose       0.91      0.86      0.89       144\n   sunflower       0.92      0.94      0.93       129\n       tulip       0.84      0.91      0.87       175\n\n    accuracy                           0.91       774\n   macro avg       0.92      0.91      0.92       774\nweighted avg       0.92      0.91      0.92       774\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\n\n# Assuming you have a CSV with columns 'filename' and 'label'\nlabels_df = pd.read_csv('/kaggle/input/flower-test-file/truelabel.csv')\n\n# Directory containing all test images\ntest_dir = '/kaggle/input/flower-recognition-dataset/flowers1/test_images'\n\n# Prepare the images\ndef load_and_preprocess_image(img_path):\n    img = image.load_img(img_path, target_size=(224, 224))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    return preprocess_input(img_array)\n\n# Load all images and their labels\ntest_images = []\nlabels = []\nfor _, row in labels_df.iterrows():\n    img_path = os.path.join(test_dir, row['image_id'])\n    img = load_and_preprocess_image(img_path)\n    test_images.append(img)\n    labels.append(row['true_label'])\n\n# Convert to numpy arrays\ntest_images = np.vstack(test_images)  # Shape will be (num_images, 224, 224, 3)\nlabels = np.array(labels)  # Make sure this is numeric or one-hot encoded as needed\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T19:34:26.323373Z","iopub.execute_input":"2024-09-02T19:34:26.323752Z","iopub.status.idle":"2024-09-02T19:34:26.757662Z","shell.execute_reply.started":"2024-09-02T19:34:26.323719Z","shell.execute_reply":"2024-09-02T19:34:26.756689Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(test_images)\npredicted_class_indices = np.argmax(predictions, axis=1)\n\n# If labels are not numeric, convert them to numeric using the same mapping used during training\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nencoder.fit(labels_df['true_label'])  # Assuming all labels are here\nnumeric_labels = encoder.transform(labels)\n\n# Metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nprint(\"Accuracy:\", accuracy_score(numeric_labels, predicted_class_indices))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(numeric_labels, predicted_class_indices))\nprint(\"Classification Report:\")\nprint(classification_report(numeric_labels, predicted_class_indices, target_names=encoder.classes_))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T19:34:52.682279Z","iopub.execute_input":"2024-09-02T19:34:52.683256Z","iopub.status.idle":"2024-09-02T19:34:53.344361Z","shell.execute_reply.started":"2024-09-02T19:34:52.683215Z","shell.execute_reply":"2024-09-02T19:34:53.343315Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\nAccuracy: 0.875\nConfusion Matrix:\n[[17  0  0  1  2]\n [ 0 19  0  0  0]\n [ 0  0 16  0  8]\n [ 0  1  0 18  0]\n [ 0  0  0  1 21]]\nClassification Report:\n              precision    recall  f1-score   support\n\n       daisy       1.00      0.85      0.92        20\n   dandelion       0.95      1.00      0.97        19\n        rose       1.00      0.67      0.80        24\n   sunflower       0.90      0.95      0.92        19\n       tulip       0.68      0.95      0.79        22\n\n    accuracy                           0.88       104\n   macro avg       0.91      0.88      0.88       104\nweighted avg       0.90      0.88      0.88       104\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.regularizers import l2\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n\nfor layer in base_model.layers[:-50]:\n    layer.trainable = True\n\n\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(256, activation='relu')(x) \nx = Dropout(0.5)(x) \npredictions = Dense(train_generator.num_classes, activation='softmax')(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T21:19:30.898476Z","iopub.execute_input":"2024-09-02T21:19:30.898886Z","iopub.status.idle":"2024-09-02T21:19:33.793148Z","shell.execute_reply.started":"2024-09-02T21:19:30.898849Z","shell.execute_reply":"2024-09-02T21:19:33.792341Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n\n# Instantiate the optimizer with the desired learning rate\noptimizer = Adam(learning_rate=0.0001)\n\n# Compile the model with the optimizer\nmodel.compile(\n    optimizer=optimizer,\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T21:19:36.346245Z","iopub.execute_input":"2024-09-02T21:19:36.347007Z","iopub.status.idle":"2024-09-02T21:19:36.363427Z","shell.execute_reply.started":"2024-09-02T21:19:36.346962Z","shell.execute_reply":"2024-09-02T21:19:36.362393Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom sklearn.utils.class_weight import compute_class_weight\n\nclass_weights = compute_class_weight('balanced', classes=np.unique(train_generator.classes), y=train_generator.classes)\nclass_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-4)\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\nsteps_per_epoch = max(1, train_generator.samples // batch_size)\nvalidation_steps = max(1, validation_set.samples // batch_size)\n\n\nmodel.fit(\n    train_generator,\n    validation_data=validation_set,\n    epochs=30, \n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n     class_weight=class_weights_dict,\n    callbacks=[early_stopping, reduce_lr]\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T21:19:41.136075Z","iopub.execute_input":"2024-09-02T21:19:41.136476Z","iopub.status.idle":"2024-09-02T21:24:12.579592Z","shell.execute_reply.started":"2024-09-02T21:19:41.136438Z","shell.execute_reply":"2024-09-02T21:24:12.578546Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1725312016.007450     119 service.cc:145] XLA service 0x7a6440002700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1725312016.007502     119 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1725312016.007506     119 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1725312042.482440     119 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 430ms/step - accuracy: 0.5151 - loss: 1.3330 - val_accuracy: 0.2620 - val_loss: 2.4010 - learning_rate: 1.0000e-04\nEpoch 2/30\n\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 583ms/step - accuracy: 0.8240 - loss: 0.4878 - val_accuracy: 0.2308 - val_loss: 1.8335 - learning_rate: 1.0000e-04\nEpoch 3/30\n\u001b[1m 2/80\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 146ms/step - accuracy: 0.9062 - loss: 0.2812","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9367 - loss: 0.2057 - val_accuracy: 0.0714 - val_loss: 2.0439 - learning_rate: 1.0000e-04\nEpoch 4/30\n\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 282ms/step - accuracy: 0.8681 - loss: 0.3954 - val_accuracy: 0.2596 - val_loss: 3.8208 - learning_rate: 1.0000e-04\nEpoch 5/30\n\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 254ms/step - accuracy: 0.8704 - loss: 0.3627 - val_accuracy: 0.2452 - val_loss: 2.0707 - learning_rate: 1.0000e-04\nEpoch 6/30\n\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9691 - loss: 0.1533 - val_accuracy: 0.3214 - val_loss: 1.7342 - learning_rate: 1.0000e-04\nEpoch 7/30\n\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 253ms/step - accuracy: 0.9136 - loss: 0.2697 - val_accuracy: 0.2572 - val_loss: 2.0800 - learning_rate: 1.0000e-04\nEpoch 8/30\n\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 244ms/step - accuracy: 0.8803 - loss: 0.3108 - val_accuracy: 0.1971 - val_loss: 1.8582 - learning_rate: 1.0000e-04\nEpoch 9/30\n\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9051 - loss: 0.2536 - val_accuracy: 0.0714 - val_loss: 2.0006 - learning_rate: 1.0000e-04\nEpoch 10/30\n\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 250ms/step - accuracy: 0.9168 - loss: 0.2400 - val_accuracy: 0.2500 - val_loss: 2.8805 - learning_rate: 1.0000e-04\nEpoch 11/30\n\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 236ms/step - accuracy: 0.9181 - loss: 0.2236 - val_accuracy: 0.1659 - val_loss: 2.0085 - learning_rate: 1.0000e-04\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7a60b4256bf0>"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\n\n# Assuming you have a CSV with columns 'filename' and 'label'\nlabels_df = pd.read_csv('/kaggle/input/flower-test-file/truelabel.csv')\n\n# Directory containing all test images\ntest_dir = '/kaggle/input/flower-recognition-dataset/flowers1/test_images'\n\n# Prepare the images\ndef load_and_preprocess_image(img_path):\n    img = image.load_img(img_path, target_size=(224, 224))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    return preprocess_input(img_array)\n\n# Load all images and their labels\ntest_images = []\nlabels = []\nfor _, row in labels_df.iterrows():\n    img_path = os.path.join(test_dir, row['image_id'])\n    img = load_and_preprocess_image(img_path)\n    test_images.append(img)\n    labels.append(row['true_label'])\n\n# Convert to numpy arrays\ntest_images = np.vstack(test_images)  # Shape will be (num_images, 224, 224, 3)\nlabels = np.array(labels)  # Make sure this is numeric or one-hot encoded as needed\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T21:34:41.089602Z","iopub.execute_input":"2024-09-02T21:34:41.090795Z","iopub.status.idle":"2024-09-02T21:34:42.365703Z","shell.execute_reply.started":"2024-09-02T21:34:41.090752Z","shell.execute_reply":"2024-09-02T21:34:42.364641Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(test_images)\npredicted_class_indices = np.argmax(predictions, axis=1)\n\n# If labels are not numeric, convert them to numeric using the same mapping used during training\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nencoder.fit(labels_df['true_label'])  # Assuming all labels are here\nnumeric_labels = encoder.transform(labels)\n\n# Metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nprint(\"Accuracy:\", accuracy_score(numeric_labels, predicted_class_indices))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(numeric_labels, predicted_class_indices))\nprint(\"Classification Report:\")\nprint(classification_report(numeric_labels, predicted_class_indices, target_names=encoder.classes_))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T21:35:10.439470Z","iopub.execute_input":"2024-09-02T21:35:10.440174Z","iopub.status.idle":"2024-09-02T21:35:20.752206Z","shell.execute_reply.started":"2024-09-02T21:35:10.440134Z","shell.execute_reply":"2024-09-02T21:35:20.751255Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step\nAccuracy: 0.8365384615384616\nConfusion Matrix:\n[[18  1  1  0  0]\n [ 0 19  0  0  0]\n [ 1  0 21  0  2]\n [ 0  5  2 10  2]\n [ 1  0  2  0 19]]\nClassification Report:\n              precision    recall  f1-score   support\n\n       daisy       0.90      0.90      0.90        20\n   dandelion       0.76      1.00      0.86        19\n        rose       0.81      0.88      0.84        24\n   sunflower       1.00      0.53      0.69        19\n       tulip       0.83      0.86      0.84        22\n\n    accuracy                           0.84       104\n   macro avg       0.86      0.83      0.83       104\nweighted avg       0.86      0.84      0.83       104\n\n","output_type":"stream"}]}]}